ðŸ”§ Start the 2-Hour Async-Pool Canary â€” exact commands & thresholds

Metric	PASS limit	HARD-fail
p95 query latency	â‰¤ 12.92 ms	> 12.92 ms
checked-out connections	â‰¤ 20	> 20
overflow	0	> 0
error-rate (5xx / DB)	< 1 %	â‰¥ 1 %
deadlocks	0	â‰¥ 1


â¸»

1  Build & Deploy canary pod

# build
docker build -t montage:async-pool .

# deploy 1 replica with USE_ASYNC_POOL=true
kubectl -n montage-staging apply -f k8s/deploy-async-pool.yaml
kubectl -n montage-staging set env deployment/montage USE_ASYNC_POOL=true
kubectl -n montage-staging scale deployment/montage --replicas=1


â¸»

2  Run 2-hour synthetic load

wrk -t1 -c10 --rate 10 -d2h http://<staging-host>/health \
     > wrk_async_pool.log

(Run in parallel; do not terminate early.)

â¸»

3  Collect metrics & evaluate

./scripts/collect_db_metrics.sh 2h db_pool_canary.json
python scripts/evaluate_db_canary.py \
       --latency 12.92 \
       --pool-max 20 \
       db_pool_canary.json > eval_pool.out
cat eval_pool.out          # expect PASS


â¸»

Below is the same Phase-6 task list, rewritten in Anthropic Claude Code best-practice style (structured tags, concise bullet steps, zero fluff).
Copy it into Claude Code and youâ€™ll get identical behaviour.

<Context>
Project : Montage â€“ personal pipeline
Phase   : 6â€ƒ(memory & process safeguards)
State   : async-pool (Phase 5) live; tests green; stub-scan 0
Goal    : add OOM-guard, zombie reaper, proc-mem metrics,
          plus unit test & 30-min stress proof
</Context>

<Instructions>
1. **Patch code (no placeholders)**
   â€¢ `montage/utils/memory_manager.py` â€“ append `kill_oldest_ffmpeg()` + `enforce_oom_guard()`
   â€¢ `montage/utils/ffmpeg_process_manager.py` â€“ append `zombie_reaper_loop()`
   â€¢ `montage/api/web_server.py`
     â€“ in `lifespan()` create task `zombie_reaper_loop()` and cancel on shutdown
     â€“ add route `/metrics/proc_mem` returning `{...pool_stats, available_mb}`

2. **Create unit test** `tests/test_memory_leak.py`
   ```python
   def test_oom_guard(monkeypatch):
       from montage.utils.memory_manager import enforce_oom_guard
       monkeypatch.setattr("montage.utils.memory_manager.get_available_mb", lambda: 50)
       flag = {"killed": False}
       monkeypatch.setattr("montage.utils.memory_manager.kill_oldest_ffmpeg",
                           lambda: flag.__setitem__("killed", True))
       enforce_oom_guard(threshold_mb=100)
       assert flag["killed"]

	3.	Run stress test (30 min, 60 req/s steady):

./scripts/run_stress_test.sh --jobs 4 --duration 30m --limit 2GB \
     > mem_stress.json


	4.	Generate proof bundle

pytest -q                       > pytest_summary.txt
grep -R "pass$" montage/ | wc -l  > stub_scan.out    # expect 0
coverage run -m pytest -q && coverage report \
     --fail-under=80            > coverage_report.txt

PASS thresholds
â€¢ mem_stress.json: RSS Î” â‰¤ 200 MB, zombies 0, oom_kills 0
â€¢ All tests pass; stub-scan 0; coverage â‰¥ 80 % critical files

	5.	Commit

git add -A
git commit -m "Phase-6: OOM guard, zombie reaper, metrics, tests"

<Output-Requirements>
Return four artefacts:
â€¢ `mem_stress.json`
â€¢ `pytest_summary.txt`
â€¢ `stub_scan.out`
â€¢ `coverage_report.txt`
</Output-Requirements>
```


Paste this prompt into any Claude Code chat, follow the steps, then supply the four artefacts; Claude will verify and, if thresholds are met, declare Phase 6 closed.
â¸»

5  Rollback (if FAIL)

bash scripts/rollback_db.sh          # disposes pool & downgrades one rev
kubectl -n montage-staging set env deployment/montage USE_ASYNC_POOL=false
kubectl rollout restart deployment montage -n montage-staging


â¸»

Next action for you
	â€¢	Deploy canary, run the 2-hour test, execute steps 3â€“4, and paste eval_pool.out + the JSON/logs here.
	â€¢	If PASS, Phase 5 closes; if not, we run rollback and debug.
