diff --git a/montage/core/real_whisper_transcriber.py b/montage/core/real_whisper_transcriber.py
new file mode 100644
index 0000000..babf88d
--- /dev/null
+++ b/montage/core/real_whisper_transcriber.py
@@ -0,0 +1,232 @@
+#!/usr/bin/env python3
+"""
+Real Whisper Transcription Implementation
+Uses OpenAI's Whisper model for accurate speech-to-text with timestamps
+"""
+import logging
+import subprocess
+import json
+import tempfile
+import os
+from typing import Dict, Any, List, Optional
+from pathlib import Path
+import asyncio
+
+logger = logging.getLogger(__name__)
+
+
+class RealWhisperTranscriber:
+    """Real Whisper transcription using whisper.cpp or OpenAI Whisper"""
+    
+    def __init__(self, model_size: str = "base"):
+        """
+        Initialize Whisper transcriber
+        
+        Args:
+            model_size: Model size (tiny, base, small, medium, large)
+        """
+        self.model_size = model_size
+        self.whisper_cpp_path = self._find_whisper_cpp()
+        
+    def _find_whisper_cpp(self) -> Optional[str]:
+        """Find whisper.cpp installation"""
+        # Common installation paths
+        paths = [
+            "/usr/local/bin/whisper",
+            "/opt/homebrew/bin/whisper",
+            os.path.expanduser("~/whisper.cpp/main"),
+            "./whisper.cpp/main"
+        ]
+        
+        for path in paths:
+            if os.path.exists(path):
+                logger.info(f"Found whisper.cpp at: {path}")
+                return path
+                
+        # Try system PATH
+        try:
+            result = subprocess.run(["which", "whisper"], capture_output=True, text=True)
+            if result.returncode == 0:
+                return result.stdout.strip()
+        except:
+            pass
+            
+        logger.warning("whisper.cpp not found, will use Python whisper")
+        return None
+        
+    async def transcribe(self, video_path: str) -> Dict[str, Any]:
+        """
+        Transcribe audio from video file
+        
+        Returns:
+            {
+                "text": "full transcript",
+                "segments": [
+                    {
+                        "start": 0.0,
+                        "end": 2.5,
+                        "text": "Hello world",
+                        "words": [
+                            {"word": "Hello", "start": 0.0, "end": 0.5},
+                            {"word": "world", "start": 0.6, "end": 1.2}
+                        ]
+                    }
+                ],
+                "language": "en"
+            }
+        """
+        try:
+            # Extract audio from video
+            audio_path = await self._extract_audio(video_path)
+            
+            # Try whisper.cpp first (faster)
+            if self.whisper_cpp_path:
+                return await self._transcribe_with_cpp(audio_path)
+            else:
+                return await self._transcribe_with_python(audio_path)
+                
+        except Exception as e:
+            logger.error(f"Transcription failed: {e}")
+            raise
+            
+    async def _extract_audio(self, video_path: str) -> str:
+        """Extract audio from video as WAV"""
+        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
+            audio_path = tmp.name
+            
+        cmd = [
+            "ffmpeg", "-i", video_path,
+            "-vn", "-acodec", "pcm_s16le",
+            "-ar", "16000", "-ac", "1",
+            audio_path, "-y"
+        ]
+        
+        process = await asyncio.create_subprocess_exec(
+            *cmd,
+            stdout=asyncio.subprocess.DEVNULL,
+            stderr=asyncio.subprocess.DEVNULL
+        )
+        await process.wait()
+        
+        return audio_path
+        
+    async def _transcribe_with_cpp(self, audio_path: str) -> Dict[str, Any]:
+        """Use whisper.cpp for fast transcription"""
+        with tempfile.NamedTemporaryFile(mode='w', suffix=".json", delete=False) as tmp:
+            output_path = tmp.name
+            
+        cmd = [
+            self.whisper_cpp_path,
+            "-m", f"models/ggml-{self.model_size}.bin",
+            "-f", audio_path,
+            "--output-json", output_path,
+            "--max-len", "1",  # Word-level timestamps
+            "--print-timestamps"
+        ]
+        
+        process = await asyncio.create_subprocess_exec(
+            *cmd,
+            stdout=asyncio.subprocess.PIPE,
+            stderr=asyncio.subprocess.PIPE
+        )
+        
+        stdout, stderr = await process.communicate()
+        
+        # Parse JSON output
+        with open(output_path, 'r') as f:
+            result = json.load(f)
+            
+        # Clean up
+        os.unlink(audio_path)
+        os.unlink(output_path)
+        
+        return self._format_whisper_output(result)
+        
+    async def _transcribe_with_python(self, audio_path: str) -> Dict[str, Any]:
+        """Use Python whisper library"""
+        try:
+            import whisper
+        except ImportError:
+            raise ImportError("Please install openai-whisper: pip install openai-whisper")
+            
+        # Load model
+        model = whisper.load_model(self.model_size)
+        
+        # Transcribe with word timestamps
+        result = model.transcribe(
+            audio_path,
+            word_timestamps=True,
+            task="transcribe"
+        )
+        
+        # Clean up
+        os.unlink(audio_path)
+        
+        return self._format_whisper_output(result)
+        
+    def _format_whisper_output(self, raw_result: Dict) -> Dict[str, Any]:
+        """Format Whisper output to standard format"""
+        segments = []
+        
+        for segment in raw_result.get("segments", []):
+            formatted_segment = {
+                "start": segment["start"],
+                "end": segment["end"],
+                "text": segment["text"].strip(),
+                "words": []
+            }
+            
+            # Extract word-level timestamps if available
+            if "words" in segment:
+                for word in segment["words"]:
+                    formatted_segment["words"].append({
+                        "word": word["word"].strip(),
+                        "start": word["start"],
+                        "end": word["end"],
+                        "confidence": word.get("probability", 1.0)
+                    })
+                    
+            segments.append(formatted_segment)
+            
+        return {
+            "text": raw_result.get("text", ""),
+            "segments": segments,
+            "language": raw_result.get("language", "en"),
+            "duration": segments[-1]["end"] if segments else 0
+        }
+        
+
+class UltraAccurateWhisperTranscriber(RealWhisperTranscriber):
+    """Ultra-accurate Whisper using large-v3 model"""
+    
+    def __init__(self):
+        super().__init__(model_size="large-v3")
+        
+    async def transcribe(self, video_path: str) -> Dict[str, Any]:
+        """Transcribe with maximum accuracy"""
+        # Use large model with enhanced settings
+        result = await super().transcribe(video_path)
+        
+        # Post-process for better accuracy
+        result["segments"] = self._enhance_accuracy(result["segments"])
+        
+        return result
+        
+    def _enhance_accuracy(self, segments: List[Dict]) -> List[Dict]:
+        """Apply post-processing for better accuracy"""
+        enhanced = []
+        
+        for segment in segments:
+            # Remove filler words and clean up
+            text = segment["text"]
+            text = text.replace(" um ", " ")
+            text = text.replace(" uh ", " ")
+            
+            # Fix common transcription errors
+            text = text.replace(" gonna ", " going to ")
+            text = text.replace(" wanna ", " want to ")
+            
+            segment["text"] = text
+            enhanced.append(segment)
+            
+        return enhanced
\ No newline at end of file
