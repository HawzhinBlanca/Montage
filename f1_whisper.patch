diff --git a/montage/core/real_whisper_transcriber.py b/montage/core/real_whisper_transcriber.py
new file mode 100644
index 0000000..1234567
--- /dev/null
+++ b/montage/core/real_whisper_transcriber.py
@@ -0,0 +1,160 @@
+#!/usr/bin/env python3
+"""
+Real Whisper Transcription Implementation
+Uses faster-whisper for efficient speech-to-text
+"""
+import os
+import logging
+import tempfile
+import subprocess
+from pathlib import Path
+from typing import Dict, Any, List, Optional
+import json
+
+logger = logging.getLogger(__name__)
+
+try:
+    import faster_whisper
+    WHISPER_AVAILABLE = True
+except ImportError:
+    WHISPER_AVAILABLE = False
+    logger.warning("faster-whisper not available. Install with: pip install faster-whisper")
+
+
+class RealWhisperTranscriber:
+    """Real Whisper transcription using faster-whisper"""
+    
+    def __init__(self, model_size: str = "base", device: str = "auto"):
+        """
+        Initialize Whisper transcriber
+        
+        Args:
+            model_size: Model size (tiny, base, small, medium, large-v3)
+            device: Device to use (auto, cpu, cuda)
+        """
+        self.model_size = model_size
+        self.device = device if device != "auto" else self._detect_device()
+        self.model = None
+        
+        # Check for model path override
+        self.model_path = os.environ.get("WHISPER_MODEL_PATH")
+        
+    def _detect_device(self) -> str:
+        """Detect best available device"""
+        try:
+            import torch
+            if torch.cuda.is_available():
+                return "cuda"
+        except ImportError:
+            pass
+        return "cpu"
+        
+    def _load_model(self):
+        """Lazy load the model"""
+        if self.model is None:
+            if not WHISPER_AVAILABLE:
+                raise ImportError("faster-whisper is required for transcription")
+                
+            compute_type = "float16" if self.device == "cuda" else "int8"
+            
+            # Use custom model path if provided
+            if self.model_path and os.path.exists(self.model_path):
+                model_path = self.model_path
+            else:
+                model_path = self.model_size
+                
+            self.model = faster_whisper.WhisperModel(
+                model_path,
+                device=self.device,
+                compute_type=compute_type
+            )
+            logger.info(f"Loaded Whisper model: {self.model_size} on {self.device}")
+    
+    def transcribe(self, video_path: str) -> Dict[str, Any]:
+        """
+        Transcribe audio from video file
+        
+        Returns:
+            {
+                "segments": [...],
+                "text": "full transcript",
+                "language": "en",
+                "duration": 123.45
+            }
+        """
+        if not Path(video_path).exists():
+            raise FileNotFoundError(f"Video not found: {video_path}")
+            
+        # Extract audio
+        audio_path = self._extract_audio(video_path)
+        
+        try:
+            # Load model if needed
+            self._load_model()
+            
+            # Transcribe
+            segments, info = self.model.transcribe(
+                audio_path,
+                beam_size=5,
+                best_of=5,
+                vad_filter=True,
+                vad_parameters=dict(
+                    min_silence_duration_ms=500
+                )
+            )
+            
+            # Format results
+            result_segments = []
+            full_text = []
+            
+            for segment in segments:
+                seg_dict = {
+                    "start": segment.start,
+                    "end": segment.end,
+                    "text": segment.text.strip()
+                }
+                
+                # Add word-level timestamps if available
+                if hasattr(segment, 'words') and segment.words:
+                    seg_dict["words"] = [
+                        {
+                            "word": word.word,
+                            "start": word.start,
+                            "end": word.end,
+                            "probability": word.probability
+                        }
+                        for word in segment.words
+                    ]
+                    
+                result_segments.append(seg_dict)
+                full_text.append(segment.text.strip())
+                
+            return {
+                "segments": result_segments,
+                "text": " ".join(full_text),
+                "language": info.language,
+                "duration": info.duration or 0
+            }
+            
+        finally:
+            # Cleanup
+            if os.path.exists(audio_path):
+                os.unlink(audio_path)
+                
+    def _extract_audio(self, video_path: str) -> str:
+        """Extract audio from video as WAV"""
+        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
+            audio_path = tmp.name
+            
+        cmd = [
+            "ffmpeg", "-i", video_path,
+            "-vn", "-acodec", "pcm_s16le",
+            "-ar", "16000", "-ac", "1",
+            audio_path, "-y"
+        ]
+        
+        subprocess.run(cmd, capture_output=True, check=True)
+        return audio_path
diff --git a/montage/core/analyze_video.py b/montage/core/analyze_video.py
index 1234567..abcdefg 100644
--- a/montage/core/analyze_video.py
+++ b/montage/core/analyze_video.py
@@ -1,6 +1,8 @@
 import cv2
 import numpy as np
 from pathlib import Path
 from typing import List, Dict
+import logging
+from .real_whisper_transcriber import RealWhisperTranscriber
+
+logger = logging.getLogger(__name__)
 
 SCENE_THRESHOLD = 30.0  # adjust per your content
 
@@ -65,16 +67,61 @@ def analyze_video(video_path: str, use_premium: bool = False) -> Dict:
-    """Wrapper for backward compatibility with celery tasks"""
+    """Main video analysis with real transcription"""
     content = analyze_video_content(video_path)
-    # Generate simple highlights based on scene changes
-    highlights = []
-    for i, scene_idx in enumerate(content["scene_changes"][:5]):  # Max 5 highlights
-        highlights.append({
-            "start": float(scene_idx),
-            "end": float(scene_idx + 5.0),
-            "score": 0.8 - (i * 0.1),  # Decreasing scores
-            "type": "scene_change"
-        })
+    
+    # Real transcription
+    try:
+        transcriber = RealWhisperTranscriber(model_size="base")
+        transcript_result = transcriber.transcribe(video_path)
+        
+        # Extract transcript and words
+        transcript = transcript_result.get("segments", [])
+        words = []
+        for segment in transcript:
+            if "words" in segment:
+                words.extend(segment["words"])
+                
+        # Generate smart highlights based on transcript
+        highlights = []
+        if transcript:
+            # Use transcript for intelligent highlight selection
+            for i, segment in enumerate(transcript[:5]):  # Top 5 segments
+                if len(segment["text"].split()) > 5:  # Meaningful segments
+                    highlights.append({
+                        "start": segment["start"],
+                        "end": segment["end"],
+                        "score": 0.9 - (i * 0.1),
+                        "text": segment["text"][:100],
+                        "type": "transcript_based"
+                    })
+        else:
+            # Fallback to scene-based highlights
+            for i, scene_idx in enumerate(content["scene_changes"][:5]):
+                highlights.append({
+                    "start": float(scene_idx),
+                    "end": float(scene_idx + 5.0),
+                    "score": 0.8 - (i * 0.1),
+                    "type": "scene_change"
+                })
+                
+    except Exception as e:
+        logger.warning(f"Transcription failed: {e}, using scene-based highlights")
+        transcript = []
+        words = []
+        # Use scene-based highlights as fallback
+        highlights = []
+        for i, scene_idx in enumerate(content["scene_changes"][:5]):
+            highlights.append({
+                "start": float(scene_idx),
+                "end": float(scene_idx + 5.0),
+                "score": 0.8 - (i * 0.1),
+                "type": "scene_change"
+            })
+    
     return {
         "highlights": highlights,
-        "analysis": content
+        "analysis": content,
+        "transcript": transcript,
+        "words": words,
+        "speaker_turns": []  # Will be implemented in F2
     }
diff --git a/tests/test_whisper_transcriber.py b/tests/test_whisper_transcriber.py
new file mode 100644
index 0000000..1234567
--- /dev/null
+++ b/tests/test_whisper_transcriber.py
@@ -0,0 +1,40 @@
+import os
+import pytest
+from montage.core.real_whisper_transcriber import RealWhisperTranscriber
+
+@pytest.mark.skipif("WHISPER_MODEL_PATH" not in os.environ, reason="model not cached")
+def test_whisper_transcription():
+    """Test real Whisper transcription"""
+    transcriber = RealWhisperTranscriber(model_size="tiny")
+    
+    # Create a test audio file
+    test_file = "test_audio.wav"
+    os.system(f'ffmpeg -f lavfi -i "sine=frequency=440:duration=2" -ar 16000 {test_file} -y 2>/dev/null')
+    
+    try:
+        result = transcriber.transcribe(test_file)
+        
+        assert "segments" in result
+        assert "text" in result
+        assert "language" in result
+        assert "duration" in result
+        assert result["duration"] > 0
+        
+    finally:
+        if os.path.exists(test_file):
+            os.unlink(test_file)
+            
+def test_whisper_init():
+    """Test Whisper initialization"""
+    transcriber = RealWhisperTranscriber()
+    assert transcriber.model_size == "base"
+    assert transcriber.device in ["cpu", "cuda"]
+    
+def test_whisper_file_not_found():
+    """Test error handling for missing file"""
+    transcriber = RealWhisperTranscriber()
+    
+    with pytest.raises(FileNotFoundError):
+        transcriber.transcribe("nonexistent.mp4")
diff --git a/pyproject.toml b/pyproject.toml
index 1234567..abcdefg 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -45,6 +45,7 @@ python-jose = "^3.3.0"
 passlib = "^1.7.4"
 python-multipart = "^0.0.6"
 httpx = "^0.24.1"
+faster-whisper = {version = "^1.1.1", extras = ["cpu"]}
 
 [tool.poetry.group.dev.dependencies]
 pytest = "^7.3.1"