#!/usr/bin/env python3
"""
Phase 1.1: Ensemble ASR + diarisation (Fixed version)
Handles cache directory and permission issues
"""

import os
import sys
import json
import logging
import asyncio
import tempfile
import time
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
import numpy as np
import subprocess

# Set cache directory to avoid permission issues
cache_dir = os.path.expanduser("~/.cache/huggingface")
os.makedirs(cache_dir, exist_ok=True)
os.environ["TRANSFORMERS_CACHE"] = cache_dir
os.environ["HF_HOME"] = cache_dir

# Critical imports
WHISPER_AVAILABLE = False
PYANNOTE_AVAILABLE = False
HTTPX_AVAILABLE = False

try:
    import faster_whisper
    WHISPER_AVAILABLE = True
    print("âœ… faster-whisper available")
except ImportError:
    print("âš ï¸  faster-whisper not available")

try:
    import pyannote.audio
    PYANNOTE_AVAILABLE = True
    print("âœ… pyannote.audio available")
except ImportError:
    print("âš ï¸  pyannote.audio not available")

try:
    import httpx
    HTTPX_AVAILABLE = True
    print("âœ… httpx available")
except ImportError:
    print("âš ï¸  httpx not available")

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class WordTiming:
    """Word with timing information"""
    word: str
    start: float
    end: float
    confidence: float
    speaker: Optional[str] = None

@dataclass
class TranscriptSegment:
    """Transcript segment with speaker info"""
    text: str
    start: float
    end: float
    speaker: str
    confidence: float
    words: List[WordTiming]

@dataclass
class EnsembleResult:
    """Complete ensemble ASR result"""
    transcript: str
    segments: List[TranscriptSegment]
    word_timings: List[WordTiming]
    speakers: List[str]
    language: str
    confidence: float
    processing_time: float
    methods_used: List[str]

class LocalWhisperProcessor:
    """Local Whisper processor with fixed cache handling"""
    
    def __init__(self, model_size: str = "base"):
        self.model_size = model_size
        self.model = None
        self.cache_dir = cache_dir
        self._init_model()
    
    def _init_model(self):
        """Initialize Whisper model with proper cache handling"""
        if not WHISPER_AVAILABLE:
            logger.error("faster-whisper not available")
            return
        
        try:
            # Create model with explicit cache directory
            self.model = faster_whisper.WhisperModel(
                self.model_size,
                device="cpu",
                compute_type="int8",
                download_root=self.cache_dir
            )
            logger.info(f"âœ… Whisper {self.model_size} model loaded")
        except Exception as e:
            logger.error(f"âŒ Whisper model loading failed: {e}")
            # Try fallback - use smaller model if base fails
            try:
                self.model = faster_whisper.WhisperModel(
                    "tiny",
                    device="cpu",
                    compute_type="int8",
                    download_root=self.cache_dir
                )
                logger.info("âœ… Whisper tiny model loaded as fallback")
            except Exception as e2:
                logger.error(f"âŒ Whisper fallback failed: {e2}")
                self.model = None
    
    def transcribe(self, audio_path: str) -> Dict[str, Any]:
        """Transcribe audio with Whisper"""
        if not self.model:
            logger.error("âŒ No Whisper model available")
            return {"error": "Model not available", "method": "whisper_local"}
        
        logger.info("ğŸ¤ Transcribing with Whisper...")
        start_time = time.time()
        
        try:
            # Transcribe with word-level timestamps
            segments, info = self.model.transcribe(
                audio_path,
                beam_size=5,
                word_timestamps=True,
                language="en"
            )
            
            # Process segments
            transcript_segments = []
            all_words = []
            full_text = []
            
            for segment in segments:
                segment_text = segment.text.strip()
                if segment_text:
                    # Extract word timings
                    words = []
                    if hasattr(segment, 'words') and segment.words:
                        for word in segment.words:
                            word_timing = WordTiming(
                                word=word.word,
                                start=word.start,
                                end=word.end,
                                confidence=word.probability
                            )
                            words.append(word_timing)
                            all_words.append(word_timing)
                    
                    # Create transcript segment
                    transcript_segment = TranscriptSegment(
                        text=segment_text,
                        start=segment.start,
                        end=segment.end,
                        speaker="SPEAKER_0",
                        confidence=segment.avg_logprob,
                        words=words
                    )
                    transcript_segments.append(transcript_segment)
                    full_text.append(segment_text)
            
            processing_time = time.time() - start_time
            
            result = {
                "success": True,
                "transcript": " ".join(full_text),
                "segments": transcript_segments,
                "words": all_words,
                "language": info.language,
                "confidence": info.language_probability,
                "processing_time": processing_time,
                "method": "whisper_local"
            }
            
            logger.info(f"âœ… Whisper transcription complete: {processing_time:.2f}s")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Whisper transcription failed: {e}")
            return {"error": str(e), "method": "whisper_local"}

class MockDeepgramProcessor:
    """Mock Deepgram processor for testing"""
    
    def __init__(self):
        self.name = "mock_deepgram"
    
    def transcribe(self, audio_path: str) -> Dict[str, Any]:
        """Mock Deepgram transcription"""
        logger.info("ğŸ­ Mock Deepgram transcription...")
        start_time = time.time()
        
        try:
            # Get audio duration
            cmd = ['ffprobe', '-v', 'error', '-show_entries', 'format=duration', 
                   '-of', 'default=noprint_wrappers=1:nokey=1', audio_path]
            result = subprocess.run(cmd, capture_output=True, text=True)
            duration = float(result.stdout.strip()) if result.returncode == 0 else 10.0
            
            # Create mock transcript
            mock_segments = []
            mock_words = []
            
            # Generate mock content based on duration
            num_segments = max(1, int(duration / 5))  # One segment per 5 seconds
            
            for i in range(num_segments):
                start = i * (duration / num_segments)
                end = (i + 1) * (duration / num_segments)
                
                # Mock words
                words = []
                mock_text = f"This is segment {i+1} of the audio transcription"
                word_list = mock_text.split()
                
                for j, word in enumerate(word_list):
                    word_start = start + (j * (end - start) / len(word_list))
                    word_end = word_start + (end - start) / len(word_list)
                    
                    word_timing = WordTiming(
                        word=word,
                        start=word_start,
                        end=word_end,
                        confidence=0.9,
                        speaker=f"SPEAKER_{i % 2}"
                    )
                    words.append(word_timing)
                    mock_words.append(word_timing)
                
                # Create segment
                segment = TranscriptSegment(
                    text=mock_text,
                    start=start,
                    end=end,
                    speaker=f"SPEAKER_{i % 2}",
                    confidence=0.9,
                    words=words
                )
                mock_segments.append(segment)
            
            processing_time = time.time() - start_time
            
            result = {
                "success": True,
                "transcript": " ".join([s.text for s in mock_segments]),
                "segments": mock_segments,
                "words": mock_words,
                "language": "en",
                "confidence": 0.9,
                "processing_time": processing_time,
                "method": "mock_deepgram"
            }
            
            logger.info(f"âœ… Mock Deepgram complete: {processing_time:.2f}s")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Mock Deepgram failed: {e}")
            return {"error": str(e), "method": "mock_deepgram"}

class SimpleROVERMerger:
    """Simplified ROVER merger"""
    
    def merge_transcriptions(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Simple merge - take best result"""
        logger.info("ğŸ”„ Simple ROVER merge...")
        
        successful_results = [r for r in results if r.get("success", False)]
        
        if not successful_results:
            logger.error("âŒ No successful transcriptions")
            return {"error": "No successful transcriptions"}
        
        if len(successful_results) == 1:
            logger.info("â„¹ï¸  Only one result - using directly")
            return successful_results[0]
        
        # Simple merge - take the one with highest confidence
        best_result = max(successful_results, key=lambda r: r.get("confidence", 0))
        
        # Add method info
        best_result["methods_used"] = [r["method"] for r in successful_results]
        best_result["method"] = "simple_merged"
        
        logger.info(f"âœ… Simple merge complete - used {best_result['method']}")
        return best_result

class EnsembleASRFixed:
    """Fixed ensemble ASR system"""
    
    def __init__(self):
        self.whisper = LocalWhisperProcessor()
        self.deepgram = MockDeepgramProcessor()
        self.rover = SimpleROVERMerger()
    
    async def process_audio(self, audio_path: str) -> EnsembleResult:
        """Process audio with ensemble pipeline"""
        logger.info("ğŸµ Starting ensemble ASR processing...")
        logger.info("=" * 60)
        
        start_time = time.time()
        
        # Run transcriptions
        results = []
        
        # Try Whisper
        if self.whisper.model:
            whisper_result = self.whisper.transcribe(audio_path)
            results.append(whisper_result)
        
        # Add mock Deepgram for testing
        mock_result = self.deepgram.transcribe(audio_path)
        results.append(mock_result)
        
        # Merge results
        merged_result = self.rover.merge_transcriptions(results)
        
        if "error" in merged_result:
            logger.error("âŒ Ensemble processing failed")
            return EnsembleResult(
                transcript="",
                segments=[],
                word_timings=[],
                speakers=[],
                language="en",
                confidence=0.0,
                processing_time=time.time() - start_time,
                methods_used=[]
            )
        
        # Build final result
        total_time = time.time() - start_time
        
        final_result = EnsembleResult(
            transcript=merged_result["transcript"],
            segments=merged_result["segments"],
            word_timings=merged_result["words"],
            speakers=list(set([s.speaker for s in merged_result["segments"]])),
            language=merged_result.get("language", "en"),
            confidence=merged_result.get("confidence", 0.8),
            processing_time=total_time,
            methods_used=merged_result.get("methods_used", [])
        )
        
        logger.info("âœ… Ensemble ASR processing complete!")
        logger.info(f"ğŸ“Š Total time: {total_time:.2f}s")
        logger.info(f"ğŸ“ Transcript length: {len(final_result.transcript)} characters")
        logger.info(f"ğŸ¯ Segments: {len(final_result.segments)}")
        logger.info(f"ğŸ‘¥ Speakers: {len(final_result.speakers)}")
        logger.info(f"ğŸ”§ Methods: {', '.join(final_result.methods_used)}")
        
        return final_result

async def main():
    """Test fixed ensemble ASR"""
    if len(sys.argv) < 2:
        print("Usage: python phase1_asr_fixed.py <audio_path>")
        return
    
    audio_path = sys.argv[1]
    
    # Initialize ensemble ASR
    ensemble = EnsembleASRFixed()
    
    # Process audio
    result = await ensemble.process_audio(audio_path)
    
    # Print results
    print("\n" + "=" * 60)
    print("ğŸ“Š ENSEMBLE ASR RESULTS")
    print("=" * 60)
    print(f"ğŸ¯ Transcript: {result.transcript[:200]}...")
    print(f"ğŸ“ Segments: {len(result.segments)}")
    print(f"ğŸ‘¥ Speakers: {result.speakers}")
    print(f"ğŸ”§ Methods: {result.methods_used}")
    print(f"â±ï¸  Processing time: {result.processing_time:.2f}s")
    
    # Save detailed results
    output_path = "ensemble_asr_fixed_result.json"
    with open(output_path, 'w') as f:
        json.dump(asdict(result), f, indent=2, default=str)
    
    print(f"ğŸ’¾ Detailed results saved to: {output_path}")

if __name__ == "__main__":
    asyncio.run(main())